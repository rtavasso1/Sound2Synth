{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from pytorch_lightning import Trainer\n",
    "from dataset import DATASET_MAPPING, DATASET_PATHS\n",
    "from interface import INTERFACE_MAPPING\n",
    "from model import Net, get_backbone, get_classifier\n",
    "from sound2synth import Sound2SynthModel\n",
    "from pathlib import Path\n",
    "from pyheaven import *\n",
    "from dataset.chains import SimpleSynth, Synplant2\n",
    "from interface.torchsynth import REG_NCLASS\n",
    "from torchsynth.config import SynthConfig\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, args):\n",
    "    interface = INTERFACE_MAPPING[args.synth]\n",
    "    net = Net(\n",
    "        backbone=get_backbone(args.backbone, args),\n",
    "        classifier=get_classifier(args.classifier, interface, args),\n",
    "    )\n",
    "    model = Sound2SynthModel.load_from_checkpoint(checkpoint_path, net=net, interface=interface, args=args)\n",
    "    return model\n",
    "\n",
    "def save_wavs(audio, path, name):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    filepath = os.path.join(path, f\"{name}.wav\")\n",
    "    torchaudio.save(filepath, audio, 48000)\n",
    "    \n",
    "def parse_outputs(outputs, unnormalizer, logits, from_0to1):\n",
    "    n_params = len(unnormalizer)\n",
    "    # print(n_params)\n",
    "    if logits:\n",
    "        outputs = torch.stack(outputs.squeeze(0).chunk(n_params)).argmax(dim=1)/(REG_NCLASS-1)\n",
    "    \n",
    "    mydict = {}\n",
    "    assert len(unnormalizer) == len(outputs), f\"Length mismatch: {len(unnormalizer)} vs {len(outputs)}\"\n",
    "    for (k, f), v in zip(unnormalizer.items(), outputs):\n",
    "        if 'keyboard' in k:\n",
    "            continue # Skip keyboard parameters as they are initialized frozen\n",
    "        mydict[k] = f.from_0to1(v.unsqueeze(0)) if from_0to1 else v.unsqueeze(0)\n",
    "    return mydict\n",
    "\n",
    "def dict_to_audio(mydict, synth):\n",
    "    synth.set_parameters(mydict)\n",
    "    # print(synth.keyboard.torchparameters.midi_f0)\n",
    "    audio = synth.output()\n",
    "    return audio\n",
    "\n",
    "# def main():\n",
    "if True:\n",
    "    checkpoint_path = './checkpoints/SimpleSynth_audioLoss/last.ckpt'  # Replace with the actual path to the checkpoint\n",
    "    output_dir = \"output_wavs\"\n",
    "    \n",
    "    args = MemberDict(dict(\n",
    "        synth=\"TorchSynth\",\n",
    "        synth_class=\"SimpleSynth\",\n",
    "        backbone=\"multimodal\",\n",
    "        classifier=\"parameter\",\n",
    "        dataset_type=\"my_multimodal\",\n",
    "        dataset=\"SimpleSynth\",\n",
    "        feature_dim=2048,\n",
    "    ))\n",
    "    \n",
    "    dataset = DATASET_MAPPING[args.dataset_type](dir='data/'+args.dataset, chain=args.synth_class, split='train')\n",
    "    \n",
    "    synth = dataset.synth\n",
    "    unnormalizer = dataset.unnormalizer\n",
    "\n",
    "    model = load_checkpoint(checkpoint_path, args)\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            if idx >= 5:  # Iterate through 5 samples\n",
    "                break\n",
    "            (features, sample_rate), label = batch\n",
    "            outputs = model(features)\n",
    "            true_params = label[0].squeeze(0)\n",
    "            # print((true_params.max() <= 1).item())\n",
    "            parsed_outputs = parse_outputs(outputs, unnormalizer, logits=True, from_0to1=True)\n",
    "            parsed_true_params = parse_outputs(true_params, unnormalizer, logits=False, from_0to1=(true_params.max() <= 1).item())\n",
    "\n",
    "            print(parsed_outputs)\n",
    "            print(parsed_true_params)\n",
    "\n",
    "            reconstructions = dict_to_audio(parsed_outputs, synth)\n",
    "            ground_truth = dict_to_audio(parsed_true_params, synth)\n",
    "            \n",
    "            # Save ground truth and reconstructions as WAV files\n",
    "            save_wavs(ground_truth, output_dir, f\"ground_truth_{idx}\")\n",
    "            save_wavs(reconstructions, output_dir, f\"reconstruction_{idx}\")\n",
    "            print(f\"Saved sample {idx}\")\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module grads, tensor([0.4724], requires_grad=True), tensor([0.4724], requires_grad=True)\n",
      "Module grads, tensor([1.], requires_grad=True), tensor([1.], requires_grad=True)\n",
      "Module grads, tensor([0.4724], requires_grad=True), tensor([0.4724], requires_grad=True)\n",
      "Module grads, tensor([1.], requires_grad=True), tensor([1.], requires_grad=True)\n",
      "Module grads, tensor([0.6508], grad_fn=<CloneBackward0>), tensor([0.6508], grad_fn=<CloneBackward0>)\n",
      "Module grads, tensor([0.5079], grad_fn=<CloneBackward0>), tensor([0.5079], grad_fn=<CloneBackward0>)\n",
      "Module grads, tensor([0.8571], grad_fn=<CloneBackward0>), tensor([0.8571], grad_fn=<CloneBackward0>)\n",
      "Module grads, tensor([0.9683], grad_fn=<CloneBackward0>), tensor([0.9683], grad_fn=<CloneBackward0>)\n",
      "Module grads, tensor([0.0794], grad_fn=<CloneBackward0>), tensor([0.0794], grad_fn=<CloneBackward0>)\n",
      "Module grads, tensor([0.5714], grad_fn=<CloneBackward0>), tensor([0.5714], grad_fn=<CloneBackward0>)\n",
      "Module grads, tensor([0.1587], grad_fn=<CloneBackward0>), tensor([0.1587], grad_fn=<CloneBackward0>)\n",
      "Module grads, tensor([1.], grad_fn=<CloneBackward0>), tensor([1.], grad_fn=<CloneBackward0>)\n",
      "Module grads, tensor([0.4127], grad_fn=<CloneBackward0>), tensor([0.4127], grad_fn=<CloneBackward0>)\n",
      "tensor([0.8471], grad_fn=<AddBackward0>)\n",
      "tensor([0.5160], grad_fn=<AddBackward0>)\n",
      "tensor([0.8571], grad_fn=<AddBackward0>)\n",
      "tensor([4.6876], grad_fn=<AddBackward0>)\n",
      "tensor([0.5683], grad_fn=<AddBackward0>)\n",
      "tensor([3.4286], grad_fn=<AddBackward0>)\n",
      "tensor([-14.2204], grad_fn=<AddBackward0>)\n",
      "tensor([3.1416], grad_fn=<AddBackward0>)\n",
      "tensor([0.4127], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from dataset.chains import SimpleSynth\n",
    "from torchsynth.config import SynthConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pyheaven import *\n",
    "from dataset import DATASET_MAPPING, DATASET_PATHS\n",
    "from interface.torchsynth import REG_NCLASS\n",
    "\n",
    "args = MemberDict(dict(\n",
    "        synth=\"TorchSynth\",\n",
    "        synth_class=\"SimpleSynth\",\n",
    "        backbone=\"multimodal\",\n",
    "        classifier=\"parameter\",\n",
    "        dataset_type=\"my_multimodal\",\n",
    "        dataset=\"SimpleSynth\",\n",
    "        feature_dim=2048,\n",
    "))\n",
    "\n",
    "dataset = DATASET_MAPPING[args.dataset_type](dir='data/'+args.dataset, chain=args.synth_class, split='train')\n",
    "\n",
    "unnormalizer = dataset.unnormalizer\n",
    "\n",
    "class DifferentiableArgmax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        idx = input.argmax(dim=1)\n",
    "        output = F.one_hot(idx, num_classes=input.size(1)).float()\n",
    "        ctx.save_for_backward(input, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, output = ctx.saved_tensors\n",
    "        grad_input = grad_output * output\n",
    "        return grad_input\n",
    "\n",
    "def parse_outputs(outputs, unnormalizer, logits, from_0to1):\n",
    "    n_params = len(unnormalizer)\n",
    "    # print(n_params)\n",
    "    if logits:\n",
    "        outputs = torch.stack(outputs.squeeze(0).chunk(n_params))\n",
    "        # print(\"Grad function after stacking:\", outputs.grad_fn)\n",
    "        \n",
    "        # Apply DifferentiableArgmax\n",
    "        outputs_one_hot = DifferentiableArgmax.apply(outputs)\n",
    "        # print(\"Grad function after DifferentiableArgmax:\", outputs_one_hot.grad_fn)\n",
    "        \n",
    "        # Convert one-hot back to index and normalize\n",
    "        outputs = outputs_one_hot.argmax(dim=1).float() / (REG_NCLASS - 1)\n",
    "        # print(\"Grad function after conversion and normalization:\", outputs.grad_fn)\n",
    "        \n",
    "        # Ensure outputs requires grad\n",
    "        if not outputs.requires_grad:\n",
    "            outputs = outputs.detach().requires_grad_()\n",
    "        \n",
    "        # print(\"Final grad function:\", outputs.grad_fn)\n",
    "    \n",
    "    mydict = {}\n",
    "    assert len(unnormalizer) == len(outputs), f\"Length mismatch: {len(unnormalizer)} vs {len(outputs)}\"\n",
    "    for (k, f), v in zip(unnormalizer.items(), outputs):\n",
    "        if 'keyboard' in k:\n",
    "            continue # Skip keyboard parameters as they are initialized frozen\n",
    "        mydict[k] = f.from_0to1(v.unsqueeze(0)) if from_0to1 else v.unsqueeze(0)\n",
    "    return mydict\n",
    "\n",
    "outputs = torch.randn((1,704), requires_grad=True)\n",
    "params = parse_outputs(outputs, unnormalizer, logits=True, from_0to1=True)\n",
    "\n",
    "synth = SimpleSynth(SynthConfig(batch_size=1, sample_rate=48000, reproducible=False, no_grad=False))\n",
    "\n",
    "synth.set_parameters(params)\n",
    "set_params = synth.get_parameters()\n",
    "for k, v in params.items():\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.unnormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def traverse_grad_fn(grad_fn):\n",
    "    if grad_fn is None:\n",
    "        return\n",
    "    ops = [grad_fn]\n",
    "    while grad_fn is not None:\n",
    "        ops.append(grad_fn)\n",
    "        if hasattr(grad_fn, 'next_functions'):\n",
    "            for f in grad_fn.next_functions:\n",
    "                if f[0] is not None:\n",
    "                    ops.extend(traverse_grad_fn(f[0]))\n",
    "        grad_fn = None\n",
    "    return ops\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.param = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.param * x\n",
    "\n",
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "c = a + b\n",
    "\n",
    "layer = CustomLayer()\n",
    "d = layer(c)\n",
    "\n",
    "type(d)\n",
    "# operations = traverse_grad_fn(d.grad_fn)\n",
    "\n",
    "# for op in operations:\n",
    "#     print(op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Signal(4.9198, grad_fn=<AliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.loss_utils import AudioLoss\n",
    "\n",
    "pred_params = torch.randn(1, 64*44)\n",
    "\n",
    "loss_fn = AudioLoss(scales=[512,1024], synth='Synplant2')\n",
    "\n",
    "loss, stfts = loss_fn(pred_params, true_params)\n",
    "# for stft in stfts:\n",
    "#     print(f'Max: {stft[0].max()}, Min: {stft[0].min()}')\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound2synth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
