{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsynth.config import SynthConfig\n",
    "from dataset import chains\n",
    "\n",
    "config = SynthConfig(batch_size=1, sample_rate=48000, reproducible=False)\n",
    "synth = chains.SimpleSynth(config)\n",
    "data = torch.load('/home/ubuntu/Sound2Synth/data/SimpleSynth/train/processed/sound_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.8038e-01,  8.0411e-01,  7.6822e-01,  3.9141e-02,  1.9138e+00,\n",
      "         6.0000e+01,  3.0000e+00, -7.2531e+00, -5.2643e-04,  2.4909e+00,\n",
      "         6.3231e-01])\n"
     ]
    }
   ],
   "source": [
    "param, _ = data['label']\n",
    "unnormalized_param = []\n",
    "for p ,(k, v) in zip(param,synth.get_parameters().items()):\n",
    "    unnormalizer = v.parameter_range\n",
    "    unnormalized = unnormalizer.from_0to1(p)\n",
    "    unnormalized_param.append(unnormalized)\n",
    "print(torch.tensor(unnormalized_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m             save_wavs(reconstructions, output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreconstruction_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 78\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m true_params \u001b[38;5;241m=\u001b[39m label[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# print((true_params.max() <= 1).item())\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m parsed_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mparse_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munnormalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_0to1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m parsed_true_params \u001b[38;5;241m=\u001b[39m parse_outputs(true_params, unnormalizer, logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, from_0to1\u001b[38;5;241m=\u001b[39m(true_params\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# print(list(unnormalizer.keys()))\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print(len(parsed_outputs), outputs.shape, true_params.shape)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# print(len(list(unnormalizer.keys())[2:]))\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# print(parsed_outputs, '\\n', parsed_true_params, '\\n', true_params, '\\n', )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mparse_outputs\u001b[0;34m(outputs, unnormalizer, logits, from_0to1)\u001b[0m\n\u001b[1;32m     34\u001b[0m mydict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unnormalizer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unnormalizer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(outputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (k, f), v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43munnormalizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m(), outputs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyboard\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip keyboard parameters as they are initialized frozen\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from pytorch_lightning import Trainer\n",
    "from dataset import DATASET_MAPPING, DATASET_PATHS\n",
    "from interface import INTERFACE_MAPPING\n",
    "from model import Net, get_backbone, get_classifier\n",
    "from sound2synth import Sound2SynthModel\n",
    "from pathlib import Path\n",
    "from pyheaven import *\n",
    "from dataset.chains import SimpleSynth, Synplant2\n",
    "from interface.torchsynth import REG_NCLASS\n",
    "from torchsynth.config import SynthConfig\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, args):\n",
    "    interface = INTERFACE_MAPPING[args.synth]\n",
    "    net = Net(\n",
    "        backbone=get_backbone(args.backbone, args),\n",
    "        classifier=get_classifier(args.classifier, interface, args),\n",
    "    )\n",
    "    model = Sound2SynthModel.load_from_checkpoint(checkpoint_path, net=net, interface=interface, args=args)\n",
    "    return model\n",
    "\n",
    "def save_wavs(audio, path, name):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    filepath = os.path.join(path, f\"{name}.wav\")\n",
    "    torchaudio.save(filepath, audio, 48000)\n",
    "    \n",
    "def parse_outputs(outputs, unnormalizer, logits, from_0to1):\n",
    "    if logits:\n",
    "        outputs = torch.stack(outputs.squeeze(0).chunk(78)).argmax(dim=1)/(REG_NCLASS-1)\n",
    "    \n",
    "    mydict = {}\n",
    "    assert len(unnormalizer) == len(outputs), f\"Length mismatch: {len(unnormalizer)} vs {len(outputs)}\"\n",
    "    for (k, f), v in zip(unnormalizer.items(), outputs):\n",
    "        if 'keyboard' in k:\n",
    "            continue # Skip keyboard parameters as they are initialized frozen\n",
    "        mydict[k] = f.from_0to1(v.unsqueeze(0)) if from_0to1 else v.unsqueeze(0)\n",
    "    return mydict\n",
    "\n",
    "def dict_to_audio(mydict, synth):\n",
    "    synth.set_parameters(mydict)\n",
    "    audio = synth.output()\n",
    "    return audio\n",
    "\n",
    "def main():\n",
    "    checkpoint_path = './checkpoints/Voice_unnorm/ckpt-epoch=01-valid_celoss=0.82.ckpt'  # Replace with the actual path to the checkpoint\n",
    "    output_dir = \"output_wavs\"\n",
    "    \n",
    "    args = MemberDict(dict(\n",
    "        synth=\"TorchSynth\",\n",
    "        synth_class=\"Voice\",\n",
    "        backbone=\"multimodal\",\n",
    "        classifier=\"parameter\",\n",
    "        dataset_type=\"my_multimodal\",\n",
    "        dataset=\"Voice\",\n",
    "        feature_dim=2048,\n",
    "    ))\n",
    "    \n",
    "    dataset = DATASET_MAPPING[args.dataset_type](dir='data/'+args.dataset, chain=args.synth_class, split='train')\n",
    "\n",
    "    unnormalizer = dataset.unnormalizer\n",
    "\n",
    "    model = load_checkpoint(checkpoint_path, args)\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            if idx >= 5:  # Iterate through 5 samples\n",
    "                break\n",
    "            (features, sample_rate), label = batch\n",
    "            outputs = model(features)\n",
    "            true_params = label[0].squeeze(0)\n",
    "            # print((true_params.max() <= 1).item())\n",
    "            parsed_outputs = parse_outputs(outputs, unnormalizer, logits=True, from_0to1=True)\n",
    "            parsed_true_params = parse_outputs(true_params, unnormalizer, logits=False, from_0to1=(true_params.max() <= 1).item())\n",
    "\n",
    "            reconstructions = dict_to_audio(parsed_outputs, synth)\n",
    "            ground_truth = dict_to_audio(parsed_true_params, synth)\n",
    "            \n",
    "            # Save ground truth and reconstructions as WAV files\n",
    "            save_wavs(ground_truth, output_dir, f\"ground_truth_{idx}\")\n",
    "            save_wavs(reconstructions, output_dir, f\"reconstruction_{idx}\")\n",
    "            print(f\"Saved sample {idx}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4724, 0.8657, 0.1759, 0.2698, 0.0317, 0.1507, 0.9320, 0.6341, 0.4901,\n",
       "        0.4556, 0.8964, 0.3074, 0.2081, 0.7231, 0.9298, 0.2437, 0.0332, 0.5263,\n",
       "        0.7423, 0.5846, 0.6323, 0.4017, 0.3489, 0.2939, 0.6977, 0.1689, 0.0223,\n",
       "        0.5185, 0.8742, 0.3971, 0.9152, 0.8000, 0.2823, 0.6816, 0.1610, 0.3051,\n",
       "        0.3734, 0.1852, 0.4194, 0.9527, 0.0362, 0.5529, 0.0885, 0.1320])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('/home/ubuntu/Sound2Synth/data/Synplant2/train/processed/sound_0.pt')['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('keyboard', 'midi_f0'), ('keyboard', 'duration'), ('adsr_1', 'attack'), ('adsr_1', 'decay'), ('adsr_1', 'sustain'), ('adsr_1', 'release'), ('adsr_1', 'alpha'), ('adsr_2', 'attack'), ('adsr_2', 'decay'), ('adsr_2', 'sustain'), ('adsr_2', 'release'), ('adsr_2', 'alpha'), ('lfo_1', 'frequency'), ('lfo_1', 'mod_depth'), ('lfo_1', 'initial_phase'), ('lfo_1', 'sin'), ('lfo_1', 'tri'), ('lfo_1', 'saw'), ('lfo_1', 'rsaw'), ('lfo_1', 'sqr'), ('lfo_2', 'frequency'), ('lfo_2', 'mod_depth'), ('lfo_2', 'initial_phase'), ('lfo_2', 'sin'), ('lfo_2', 'tri'), ('lfo_2', 'saw'), ('lfo_2', 'rsaw'), ('lfo_2', 'sqr'), ('lfo_1_amp_adsr', 'attack'), ('lfo_1_amp_adsr', 'decay'), ('lfo_1_amp_adsr', 'sustain'), ('lfo_1_amp_adsr', 'release'), ('lfo_1_amp_adsr', 'alpha'), ('lfo_2_amp_adsr', 'attack'), ('lfo_2_amp_adsr', 'decay'), ('lfo_2_amp_adsr', 'sustain'), ('lfo_2_amp_adsr', 'release'), ('lfo_2_amp_adsr', 'alpha'), ('lfo_1_rate_adsr', 'attack'), ('lfo_1_rate_adsr', 'decay'), ('lfo_1_rate_adsr', 'sustain'), ('lfo_1_rate_adsr', 'release'), ('lfo_1_rate_adsr', 'alpha'), ('lfo_2_rate_adsr', 'attack'), ('lfo_2_rate_adsr', 'decay'), ('lfo_2_rate_adsr', 'sustain'), ('lfo_2_rate_adsr', 'release'), ('lfo_2_rate_adsr', 'alpha'), ('mod_matrix', 'adsr_1->vco_1_pitch'), ('mod_matrix', 'adsr_1->vco_1_amp'), ('mod_matrix', 'adsr_1->vco_2_pitch'), ('mod_matrix', 'adsr_1->vco_2_amp'), ('mod_matrix', 'adsr_1->noise_amp'), ('mod_matrix', 'adsr_2->vco_1_pitch'), ('mod_matrix', 'adsr_2->vco_1_amp'), ('mod_matrix', 'adsr_2->vco_2_pitch'), ('mod_matrix', 'adsr_2->vco_2_amp'), ('mod_matrix', 'adsr_2->noise_amp'), ('mod_matrix', 'lfo_1->vco_1_pitch'), ('mod_matrix', 'lfo_1->vco_1_amp'), ('mod_matrix', 'lfo_1->vco_2_pitch'), ('mod_matrix', 'lfo_1->vco_2_amp'), ('mod_matrix', 'lfo_1->noise_amp'), ('mod_matrix', 'lfo_2->vco_1_pitch'), ('mod_matrix', 'lfo_2->vco_1_amp'), ('mod_matrix', 'lfo_2->vco_2_pitch'), ('mod_matrix', 'lfo_2->vco_2_amp'), ('mod_matrix', 'lfo_2->noise_amp'), ('vco_1', 'tuning'), ('vco_1', 'mod_depth'), ('vco_1', 'initial_phase'), ('vco_2', 'tuning'), ('vco_2', 'mod_depth'), ('vco_2', 'initial_phase'), ('vco_2', 'shape'), ('mixer', 'vco_1'), ('mixer', 'vco_2'), ('mixer', 'noise')]\n"
     ]
    }
   ],
   "source": [
    "from dataset.chains import Voice\n",
    "from torchsynth.config import SynthConfig\n",
    "\n",
    "params = []\n",
    "synth = Voice(SynthConfig(batch_size=1, sample_rate=48000, reproducible=False))\n",
    "for k, v in synth.named_parameters():\n",
    "    k = k.split('.')\n",
    "    k = (k[0], k[-1])\n",
    "    params.append(k)\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_wavs\"\n",
    "    \n",
    "args = MemberDict(dict(\n",
    "    synth=\"TorchSynth\",\n",
    "    backbone=\"multimodal\",\n",
    "    classifier=\"parameter\",\n",
    "    dataset_type=\"my_multimodal\",\n",
    "    dataset=\"Synplant2\",\n",
    "    feature_dim=2048,\n",
    "))\n",
    "\n",
    "dataset = DATASET_MAPPING[args.dataset_type](dir=DATASET_PATHS[args.dataset], split='test')\n",
    "\n",
    "unnormalizer = {}\n",
    "synth = SimpleSynth(SynthConfig(batch_size=1, sample_rate=48000, reproducible=False))\n",
    "for k, v in synth.named_parameters():\n",
    "    k = k.split('.')\n",
    "    k = (k[0], k[-1])\n",
    "    unnormalizer[k] = v.parameter_range\n",
    "\n",
    "model = load_checkpoint(checkpoint_path, args)\n",
    "model.eval()\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    if idx >= 50:  # Iterate through 5 samples\n",
    "        break\n",
    "    (features, sample_rate), label = batch\n",
    "\n",
    "    true_params = label[0]\n",
    "    parsed_true_params = parse_outputs(true_params, unnormalizer, logits=False, from_0to1=(true_params.max() <= 1).item())\n",
    "    ground_truth = dict_to_audio(parsed_true_params, synth)\n",
    "    \n",
    "    # Save ground truth and reconstructions as WAV files\n",
    "    save_wavs(ground_truth, output_dir, f\"ground_truth_{idx}\")\n",
    "    print(f\"Saved sample {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.chains import SimpleSynth\n",
    "from torchsynth.config import SynthConfig\n",
    "\n",
    "unnormalizer = {}\n",
    "synth = SimpleSynth(SynthConfig(batch_size=1, sample_rate=48000, reproducible=False))\n",
    "for k, v in synth.named_parameters():\n",
    "    k = k.split('.')\n",
    "    k = (k[0], k[-1])\n",
    "    unnormalizer[k] = v.parameter_range\n",
    "unnormalizer\n",
    "# next(synth.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsynth.module import MonophonicKeyboard\n",
    "from torchsynth.config import SynthConfig\n",
    "import torch\n",
    "\n",
    "params = {\"midi_f0\": torch.tensor([60.0]), \"duration\": torch.tensor([3.0])}\n",
    "keyboard = MonophonicKeyboard(SynthConfig(batch_size=1, sample_rate=48000, reproducible=False), **params)\n",
    "\n",
    "keyboard.get_parameter(\"midi_f0\").from_0to1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 44])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = []\n",
    "for n in range(5):\n",
    "    data = torch.load(f'/home/ubuntu/Sound2Synth/data/Synplant2/train/unprocessed/sound_{n}.pt')\n",
    "    params.append(data['params'])\n",
    "torch.stack(params).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound2synth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
